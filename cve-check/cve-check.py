#!/usr/bin/env python3
"""CVE Pattern Checker for h-cli Asimov Firewall.

Scans the NVD for vulnerabilities affecting the h-cli stack, then hands
all CVEs to Claude in a single batch call with WebSearch enabled so it
can research actual exploits and propose meaningful denylist patterns.

Approved patterns are written to a staging file in data/ — the user
manually appends them to blocked-patterns.txt after review.

Usage:
    docker compose --profile tools run -it --rm cve-check
    docker compose --profile tools run --rm cve-check --list
    docker compose --profile tools run --rm cve-check --dry-run

No external dependencies — stdlib only.
"""

import argparse
import json
import os
import subprocess
import sys
import time
import urllib.error
import urllib.parse
import urllib.request
from datetime import datetime, timedelta, timezone
from pathlib import Path

# ── Config ──────────────────────────────────────────────────────────────

APP_DIR = Path("/app")
BLOCKED_PATTERNS_FILE = APP_DIR / "blocked-patterns.txt"
SKILLS_DIRS = [
    APP_DIR / "skills" / "public",
    APP_DIR / "skills" / "private",
]
DATA_DIR = APP_DIR / "data"
CHECKPOINT_FILE = DATA_DIR / "cve-checkpoint.json"
STAGING_FILE = DATA_DIR / "cve-proposed-patterns.txt"

NVD_API_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0"

BASE_STACK = [
    "linux kernel",
    "ubuntu",
    "python",
    "docker",
    "redis",
    "grafana",
    "timescaledb",
    "postgresql",
    "node.js",
    "claude",
    "qdrant",
]

KEYWORD_BLOCKLIST = {
    "stats", "statistics", "tokens", "cost", "usage", "spending",
    "expensive", "cheap", "metrics", "dashboard", "how much", "budget",
    "device", "interface", "cable", "ip address", "inventory",
    "lab", "topology", "node", "console",
    "chart", "graph", "image", "panel", "picture", "png", "render",
    "screenshot", "show", "show me", "send me", "monitoring",
    "router", "telnet",
    "eve", "vip", "port forward", "port forwarding",
}

MAX_DAYS_PER_QUERY = 30
CVE_BATCH_SIZE = 10
CHECKPOINT_VERSION = 1

NVD_RATE_DELAY_NO_KEY = 6.5
NVD_RATE_DELAY_WITH_KEY = 0.7

# ── Hardcoded System Context (immutable) ────────────────────────────────
# Baked into the Docker image. The analysis model cannot modify this.

SYSTEM_CONTEXT = """\
h-cli is a natural-language infrastructure management system. A user sends
messages via Telegram, which are dispatched to Claude Code running inside a
Docker container. Claude executes commands on a ParrotOS-based core container
via MCP (Model Context Protocol).

ARCHITECTURE:
- telegram-bot: receives user messages, queues to Redis
- claude-code: dispatches tasks to Claude CLI, runs firewall proxy
- core: ParrotOS container with network/sysadmin tools, executes commands
- redis: message queue and session store
- timescaledb + grafana: monitoring stack (optional)
- qdrant: vector DB for long-term memory (optional)

ASIMOV FIREWALL (what this tool feeds):
The firewall is an MCP proxy (firewall.py) between claude-code and core.
It has two layers:
  Layer 1 — Pattern denylist (blocked-patterns.txt): deterministic,
    zero-latency substring matching against every command. If any pattern
    matches (case-insensitive), the command is blocked immediately.
  Layer 2 — Haiku gate (optional): semantic analysis by a small LLM for
    commands that pass Layer 1.
This tool maintains Layer 1 only.

LEGITIMATE COMMANDS INSIDE CORE (do NOT block these):
  nmap, tcpdump, traceroute, mtr, ping, ss, ip, iptables (scoped sudo)
  curl, wget, dig, nslookup, whois, ssh, scp, netstat
  python3, cat, ls, grep, find, apt list, systemctl status
  redis-cli (connecting to own Redis), psql (querying metrics)

WHAT TO BLOCK (patterns that indicate exploitation):
  - Reverse shell techniques (already covered, but new variants emerge)
  - Container escape commands (nsenter, mounting docker.sock, cgroup abuse)
  - Privilege escalation via tool flags (sudo bypass, nmap --interactive)
  - Data exfiltration channels (DNS tunneling, curl to external with data)
  - Tool-specific CVE exploitation (redis MODULE LOAD, docker --privileged)
  - Destructive operations (rm -rf /, disk wiping, kernel manipulation)

PATTERN FORMAT:
  Each pattern is a case-insensitive substring. When a command contains the
  pattern anywhere in it, the command is blocked. Patterns must be specific
  enough to avoid false positives on the legitimate commands listed above.
  Example: "redis-cli debug set-active-expire" blocks that specific abuse
  without blocking normal "redis-cli get mykey" operations.\
"""

# ── Token pricing (per million tokens) ──────────────────────────────────

MODEL_PRICING = {
    "claude-sonnet-4-20250514": {"input": 3.0, "output": 15.0},
    "claude-sonnet-4-6": {"input": 3.0, "output": 15.0},
}
DEFAULT_PRICING = {"input": 3.0, "output": 15.0}


# ── Checkpoint ──────────────────────────────────────────────────────────

def load_checkpoint() -> dict:
    if CHECKPOINT_FILE.exists():
        with open(CHECKPOINT_FILE) as f:
            return json.load(f)
    return {
        "last_scan_date": None,
        "processed_cve_ids": [],
        "last_run": None,
        "version": CHECKPOINT_VERSION,
    }


def save_checkpoint(checkpoint: dict) -> None:
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    checkpoint["last_run"] = datetime.now(timezone.utc).isoformat()
    with open(CHECKPOINT_FILE, "w") as f:
        json.dump(checkpoint, f, indent=2)


# ── Technology List ─────────────────────────────────────────────────────

def extract_skills_keywords() -> list[str]:
    keywords = set()
    for skills_dir in SKILLS_DIRS:
        if not skills_dir.is_dir():
            continue
        for fpath in sorted(skills_dir.glob("*.md")):
            try:
                content = fpath.read_text()
            except OSError:
                continue
            if not content.startswith("---"):
                continue
            end = content.find("---", 3)
            if end == -1:
                continue
            header = content[3:end]
            for line in header.splitlines():
                line = line.strip()
                if line.lower().startswith("keywords:"):
                    kw_str = line.split(":", 1)[1].strip()
                    for kw in kw_str.split(","):
                        kw = kw.strip().lower()
                        if kw and kw not in KEYWORD_BLOCKLIST:
                            keywords.add(kw)
    return sorted(keywords)


def build_tech_list() -> list[str]:
    techs = list(BASE_STACK)
    skills_kws = extract_skills_keywords()
    for kw in skills_kws:
        if kw not in techs:
            techs.append(kw)
    return techs


# ── NVD Queries ─────────────────────────────────────────────────────────

def _nvd_date_fmt(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%dT%H:%M:%S.000")


def _get_rate_delay() -> float:
    if os.environ.get("NVD_API_KEY"):
        return NVD_RATE_DELAY_WITH_KEY
    return NVD_RATE_DELAY_NO_KEY


def query_nvd(keyword: str, start_date: datetime, end_date: datetime) -> list[dict]:
    api_key = os.environ.get("NVD_API_KEY")
    rate_delay = _get_rate_delay()
    results = []

    for severity in ["CRITICAL", "HIGH"]:
        params = {
            "keywordSearch": keyword,
            "cvssV3Severity": severity,
            "pubStartDate": _nvd_date_fmt(start_date),
            "pubEndDate": _nvd_date_fmt(end_date),
        }
        url = f"{NVD_API_URL}?{urllib.parse.urlencode(params)}"

        headers = {}
        if api_key:
            headers["apiKey"] = api_key

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=30) as resp:
                data = json.loads(resp.read().decode())

            for vuln in data.get("vulnerabilities", []):
                cve_data = vuln.get("cve", {})
                results.append(cve_data)

        except (urllib.error.URLError, urllib.error.HTTPError, json.JSONDecodeError) as e:
            print(f"  Warning: NVD query failed for '{keyword}' ({severity}): {e}")

        time.sleep(rate_delay)

    return results


def chunk_date_range(start: datetime, end: datetime) -> list[tuple[datetime, datetime]]:
    chunks = []
    current = start
    while current < end:
        chunk_end = min(current + timedelta(days=MAX_DAYS_PER_QUERY), end)
        chunks.append((current, chunk_end))
        current = chunk_end
    return chunks


def extract_cve_info(cve_data: dict) -> tuple[str, str, float]:
    cve_id = cve_data.get("id", "UNKNOWN")

    description = ""
    for desc in cve_data.get("descriptions", []):
        if desc.get("lang") == "en":
            description = desc.get("value", "")
            break
    if not description:
        descs = cve_data.get("descriptions", [])
        if descs:
            description = descs[0].get("value", "")

    score = 0.0
    metrics = cve_data.get("metrics", {})
    for key in ["cvssMetricV31", "cvssMetricV30"]:
        metric_list = metrics.get(key, [])
        if metric_list:
            cvss = metric_list[0].get("cvssData", {})
            score = cvss.get("baseScore", 0.0)
            break

    return cve_id, description, score


# ── Claude Batch Analysis ──────────────────────────────────────────────

def build_analysis_prompt(cve_summaries: list[dict], current_patterns: str) -> str:
    cve_block = ""
    for cve in cve_summaries:
        cve_block += (
            f"\n{cve['id']} | CVSS {cve['score']} | {cve['keyword']}\n"
            f"  {cve['description'][:200]}\n"
        )

    return (
        "You are a security analyst reviewing CVEs for a command-line firewall.\n"
        "\n"
        "## System Context\n"
        f"{SYSTEM_CONTEXT}\n"
        "\n"
        f"There are {len(cve_summaries)} new CVEs to review. Most will NOT need patterns\n"
        "(memory bugs, web-only, library-level issues). Your job is to find the ones\n"
        "that DO have command-line exploitation vectors relevant to this system.\n"
        "\n"
        "## Step 1: Triage\n"
        "Scan all CVEs below. Identify any that could involve:\n"
        "- Specific CLI commands or flags that trigger the vulnerability\n"
        "- Tool abuse (redis-cli, docker, curl, nmap, etc.)\n"
        "- Shell escape sequences or injection patterns\n"
        "- Configuration file manipulation via command line\n"
        "- Container escape or privilege escalation techniques\n"
        "\n"
        "IMPORTANT: Do NOT dismiss CVEs without research. Memory corruption,\n"
        "buffer overflow, and RCE vulnerabilities often have CLI-based PoCs\n"
        "that aren't obvious from the description alone.\n"
        "\n"
        "## Step 2: Research (MANDATORY)\n"
        "You MUST use WebSearch for at least the top 3-5 most severe CVEs.\n"
        "Search for \"{CVE-ID} exploit\" or \"{CVE-ID} proof of concept\".\n"
        "For each promising CVE, look for:\n"
        "- Proof-of-concept exploits with actual commands\n"
        "- Security advisories with technical details\n"
        "- Blog posts or writeups showing exploitation steps\n"
        "\n"
        "## Step 3: Propose patterns\n"
        "For CVEs with real CLI vectors, propose specific substring patterns.\n"
        "Remember: patterns are case-insensitive substrings matched against the\n"
        "full command string. They must be specific enough to avoid blocking the\n"
        "legitimate commands listed in the system context above.\n"
        "\n"
        "## Current blocked patterns file:\n"
        "```\n"
        f"{current_patterns}\n"
        "```\n"
        "\n"
        "## CVEs to review:\n"
        f"{cve_block}\n"
        "\n"
        "## Output format\n"
        "Output ONLY a JSON array. Each element:\n"
        '{"cve_id": "CVE-...", "score": 9.8, "keyword": "redis", "patterns": ["pattern1", "pattern2"], "reason": "brief explanation"}\n'
        "\n"
        "If NO CVEs need patterns, output: []\n"
        "Do NOT include CVEs that don't need patterns.\n"
        "CRITICAL: Your final output MUST be ONLY a raw JSON array — no analysis text,\n"
        "no markdown fences, no commentary before or after. Just the JSON array.\n"
        "If no patterns needed: []\n"
        "Think through your analysis internally, but output ONLY the JSON."
    )


def run_claude_analysis(prompt: str) -> tuple[str | None, dict]:
    """Run Claude analysis. Returns (text_output, usage_stats)."""
    print("Running Claude analysis with WebSearch (this may take a few minutes)...")

    try:
        result = subprocess.run(
            [
                "claude", "-p",
                "--output-format", "json",
                "--model", os.environ.get("CVE_CHECK_MODEL", "sonnet"),
                "--dangerously-skip-permissions",
                "--allowedTools", "WebSearch",
                "--no-session-persistence",
                "--disable-slash-commands",
                "--", prompt,
            ],
            capture_output=True,
            text=True,
            timeout=600,
        )
        if result.returncode != 0:
            print(f"  Warning: Claude analysis failed (exit {result.returncode})")
            if result.stderr.strip():
                print(f"  stderr: {result.stderr.strip()[:500]}")
            if result.stdout.strip():
                print(f"  stdout: {result.stdout.strip()[:500]}")
            return None, {}

        response = json.loads(result.stdout)
        text = response.get("result", "")
        raw_usage = response.get("usage", {})
        # Model is in modelUsage dict, not top-level
        model_usage = response.get("modelUsage", {})
        model_name = list(model_usage.keys())[0] if model_usage else "unknown"
        usage = {
            "model": model_name,
            "input_tokens": raw_usage.get("input_tokens", 0)
                + raw_usage.get("cache_read_input_tokens", 0)
                + raw_usage.get("cache_creation_input_tokens", 0),
            "output_tokens": raw_usage.get("output_tokens", 0),
            "duration_ms": response.get("duration_ms", 0),
            "cost_usd": response.get("total_cost_usd", 0),
            "num_turns": response.get("num_turns", 0),
            "web_searches": raw_usage.get("server_tool_use", {}).get("web_search_requests", 0),
        }
        return text.strip(), usage

    except json.JSONDecodeError:
        print(f"  Warning: Could not parse Claude JSON response")
        return None, {}
    except FileNotFoundError:
        print("Error: 'claude' CLI not found in container.", file=sys.stderr)
        sys.exit(1)
    except subprocess.TimeoutExpired:
        print("  Warning: Claude analysis timed out (10 min limit)")
        return None, {}


def print_cost_summary(usage: dict) -> None:
    """Print a cost summary line similar to h-cli dispatcher output."""
    if not usage:
        return

    model = usage.get("model", "unknown")
    inp = usage.get("input_tokens", 0)
    out = usage.get("output_tokens", 0)
    duration_s = usage.get("duration_ms", 0) / 1000
    turns = usage.get("num_turns", 0)

    # Calculate cost from usage if not provided
    cost = usage.get("cost_usd", 0)
    if not cost and (inp or out):
        pricing = MODEL_PRICING.get(model, DEFAULT_PRICING)
        cost = (inp * pricing["input"] + out * pricing["output"]) / 1_000_000

    inp_k = f"{inp / 1000:.1f}k" if inp >= 1000 else str(inp)
    out_k = f"{out / 1000:.1f}k" if out >= 1000 else str(out)

    searches = usage.get("web_searches", 0)
    print(f"\n  {model} | in {inp_k} out {out_k} | ${cost:.4f} | {duration_s:.1f}s | {turns} turns | {searches} searches")


def parse_batch_output(output: str) -> list[dict]:
    if not output:
        return []

    text = output.strip()

    # Try direct parse first
    try:
        proposals = json.loads(text)
        if isinstance(proposals, list):
            return proposals
    except json.JSONDecodeError:
        pass

    # Strip markdown fences
    if "```" in text:
        lines = text.splitlines()
        lines = [l for l in lines if not l.startswith("```")]
        try:
            proposals = json.loads("\n".join(lines).strip())
            if isinstance(proposals, list):
                return proposals
        except json.JSONDecodeError:
            pass

    # Extract JSON array from mixed text output
    import re
    match = re.search(r'\[\s*\{.*\}\s*\]', text, re.DOTALL)
    if match:
        try:
            proposals = json.loads(match.group())
            if isinstance(proposals, list):
                return proposals
        except json.JSONDecodeError:
            pass

    # Check for empty array anywhere in text
    if "[]" in text:
        return []

    print(f"  Warning: Could not extract JSON from Claude output")
    print(f"  Raw output:\n{output[:500]}")
    return []


# ── User Interaction ────────────────────────────────────────────────────

def prompt_user(proposal: dict) -> tuple[str, list[str]]:
    cve_id = proposal.get("cve_id", "UNKNOWN")
    score = proposal.get("score", 0.0)
    keyword = proposal.get("keyword", "")
    patterns = proposal.get("patterns", [])
    reason = proposal.get("reason", "")

    sep = "=" * 70
    print(f"\n{sep}")
    print(f"  {cve_id}  (CVSS {score}, {keyword})")
    print(f"{sep}")
    if reason:
        print(f"\n  {reason}")
    print("\n  Proposed patterns:")
    for i, p in enumerate(patterns, 1):
        print(f"    {i}. {p}")
    print()

    while True:
        choice = input("  [a]pprove / [s]kip / [e]dit / [q]uit > ").strip().lower()
        if choice == "a":
            return "approve", patterns
        elif choice == "s":
            return "skip", []
        elif choice == "q":
            return "quit", []
        elif choice == "e":
            print("  Enter patterns (one per line, empty line to finish):")
            edited = []
            while True:
                line = input("    > ").strip()
                if not line:
                    break
                edited.append(line)
            if edited:
                return "approve", edited
            print("  No patterns entered, skipping.")
            return "skip", []
        else:
            print("  Invalid choice. Use a/s/e/q.")


# ── File Operations ─────────────────────────────────────────────────────

def append_staging(cve_id: str, score: float, keyword: str,
                   patterns: list[str]) -> None:
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    header = f"# -- {cve_id} ({score}, {keyword}) "
    header = header + "-" * max(1, 65 - len(header))

    with open(STAGING_FILE, "a") as f:
        f.write(f"\n{header}\n")
        for p in patterns:
            f.write(f"{p}\n")

    print(f"  + {len(patterns)} pattern(s) written to staging file")


# ── Main ────────────────────────────────────────────────────────────────

def main():
    parser = argparse.ArgumentParser(description="CVE Pattern Checker for h-cli firewall")
    parser.add_argument("--list", action="store_true", help="Show technology list and exit")
    parser.add_argument("--dry-run", action="store_true", help="Query + analyze but don't write files")
    args = parser.parse_args()

    tech_list = build_tech_list()

    if args.list:
        print("Technology list for CVE scanning:\n")
        print("Base stack:")
        for t in BASE_STACK:
            print(f"  - {t}")
        skills_kws = extract_skills_keywords()
        if skills_kws:
            print("\nFrom skills:")
            for t in skills_kws:
                marker = " (already in base)" if t in BASE_STACK else ""
                print(f"  - {t}{marker}")
        print(f"\nTotal unique technologies: {len(tech_list)}")
        return

    # Load state
    now = datetime.now(timezone.utc)
    checkpoint = load_checkpoint()
    # Reset processed list if older than scan window (avoids unbounded growth)
    last_run = checkpoint.get("last_run")
    if last_run:
        last_run_dt = datetime.fromisoformat(last_run)
        if (now - last_run_dt).days > MAX_DAYS_PER_QUERY:
            checkpoint["processed_cve_ids"] = []
            print(f"Checkpoint older than {MAX_DAYS_PER_QUERY} days — resetting processed list")
    processed = set(checkpoint.get("processed_cve_ids", []))
    current_patterns = BLOCKED_PATTERNS_FILE.read_text() if BLOCKED_PATTERNS_FILE.exists() else ""

    # Determine date range
    start_date = now - timedelta(days=MAX_DAYS_PER_QUERY)
    if processed:
        print(f"Scanning last {MAX_DAYS_PER_QUERY} days ({len(processed)} CVEs already processed)")
    else:
        print(f"First run: scanning last {MAX_DAYS_PER_QUERY} days")

    end_date = now
    date_chunks = chunk_date_range(start_date, end_date)

    has_api_key = bool(os.environ.get("NVD_API_KEY"))
    rate_info = "with API key" if has_api_key else "without API key (slower)"
    print(f"Rate limiting: {rate_info}")

    if args.dry_run:
        print("DRY RUN -- no files will be modified\n")
    else:
        print()

    # Collect all CVEs
    all_cves = {}
    print(f"Querying NVD for {len(tech_list)} technologies...")
    for i, keyword in enumerate(tech_list, 1):
        print(f"  [{i}/{len(tech_list)}] {keyword}", end="", flush=True)
        keyword_cves = []
        for chunk_start, chunk_end in date_chunks:
            results = query_nvd(keyword, chunk_start, chunk_end)
            keyword_cves.extend(results)
        new_count = 0
        for cve_data in keyword_cves:
            cve_id = cve_data.get("id", "")
            if cve_id and cve_id not in processed and cve_id not in all_cves:
                all_cves[cve_id] = (cve_data, keyword)
                new_count += 1
        print(f" -- {len(keyword_cves)} found, {new_count} new")

    if not all_cves:
        print("\nNo new CVEs to process.")
        checkpoint["last_scan_date"] = end_date.isoformat()
        if not args.dry_run:
            save_checkpoint(checkpoint)
        return

    # Build summaries for Claude
    cve_summaries = []
    for cve_id, (cve_data, keyword) in sorted(all_cves.items()):
        _, description, score = extract_cve_info(cve_data)
        cve_summaries.append({
            "id": cve_id,
            "description": description,
            "score": score,
            "keyword": keyword,
        })

    print(f"\n{len(cve_summaries)} new CVE(s) to analyze.\n")

    # Batched Claude calls with WebSearch
    proposals = []
    total_usage = {"input_tokens": 0, "output_tokens": 0, "cost_usd": 0, "duration_ms": 0, "num_turns": 0, "model": ""}
    num_batches = (len(cve_summaries) + CVE_BATCH_SIZE - 1) // CVE_BATCH_SIZE

    for batch_idx in range(num_batches):
        batch_start = batch_idx * CVE_BATCH_SIZE
        batch_end = min(batch_start + CVE_BATCH_SIZE, len(cve_summaries))
        batch = cve_summaries[batch_start:batch_end]

        print(f"\nBatch {batch_idx + 1}/{num_batches} ({len(batch)} CVEs)...")
        prompt = build_analysis_prompt(batch, current_patterns)
        output, usage = run_claude_analysis(prompt)

        if usage:
            total_usage["model"] = usage.get("model", total_usage["model"])
            for k in ["input_tokens", "output_tokens", "cost_usd", "duration_ms", "num_turns"]:
                total_usage[k] += usage.get(k, 0)

        batch_proposals = parse_batch_output(output)
        proposals.extend(batch_proposals)

        if batch_proposals:
            print(f"  Found {len(batch_proposals)} proposal(s) in this batch")
        else:
            print(f"  No patterns needed for this batch")

    print_cost_summary(total_usage)

    if not proposals:
        print("\nClaude found no CVEs requiring new patterns.")
    else:
        print(f"\nClaude proposed patterns for {len(proposals)} CVE(s) total.\n")

    # Mark all CVEs as processed
    all_cve_ids = {s["id"] for s in cve_summaries}

    if args.dry_run:
        for p in proposals:
            print(f"  DRY RUN -- {p['cve_id']} ({p.get('score', '?')}, {p.get('keyword', '?')}):")
            for pat in p.get("patterns", []):
                print(f"    {pat}")
            if p.get("reason"):
                print(f"    Reason: {p['reason']}")
        print(f"\nDone (dry run). {len(proposals)} proposal(s), {len(all_cve_ids)} CVE(s) scanned.")
        return

    # Interactive approval
    approved_count = 0
    for proposal in proposals:
        action, final_patterns = prompt_user(proposal)

        if action == "quit":
            print("\nQuitting. Progress saved.")
            processed.update(all_cve_ids)
            checkpoint["processed_cve_ids"] = sorted(processed)
            checkpoint["last_scan_date"] = end_date.isoformat()
            save_checkpoint(checkpoint)
            return
        elif action == "approve" and final_patterns:
            append_staging(
                proposal.get("cve_id", "UNKNOWN"),
                proposal.get("score", 0.0),
                proposal.get("keyword", ""),
                final_patterns,
            )
            approved_count += 1

    # Save checkpoint
    processed.update(all_cve_ids)
    checkpoint["processed_cve_ids"] = sorted(processed)
    checkpoint["last_scan_date"] = end_date.isoformat()
    save_checkpoint(checkpoint)

    print(f"\nDone. Approved: {approved_count}, Proposals: {len(proposals)}, CVEs scanned: {len(all_cve_ids)}")
    if approved_count > 0:
        print(f"\nApproved patterns written to: {STAGING_FILE}")
        print("To apply, review and append to blocked-patterns.txt:")
        print("  cat data/cve-proposed-patterns.txt >> blocked-patterns.txt")
        print("Then restart claude-code to pick up changes.")


if __name__ == "__main__":
    main()
